{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Probabilistic models\n",
    "\n",
    "* In order to use any of the previous VLCs (Variable Length Codecs), a probabilistic model is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Static models\n",
    "\n",
    "* Static models are the simplest ones because they suppose that the probabilities of the symbols\n",
    "  remain constant through the compression/decompression.\n",
    "* In this case, variable-length code-words can be precomputed.\n",
    "* Static models are very common in codecs such\n",
    "  as JPEG and MPEG (audio and video)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Adaptive models\n",
    "\n",
    "* The probabilities of the symbols are computed while they arrive to the compressor.\n",
    "\n",
    "* In general, the compression ratios that adaptive models reach are better than static model's ones, because the\n",
    "  probabilities of the symbols are localy computed (think of the sequence `aaaaaaaaaaaaaabbbbbbbbbbbbbbb`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoding\n",
    "\n",
    "1. Asign the same probability to all the symbols of the source alphabet.\n",
    "2. While the input if not exhausted:\n",
    "    1. Encode the next symbol.\n",
    "    2. Update (increase) its probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "<img src=\"data/adaptative-model-encoder.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoding\n",
    "\n",
    "1. Identical to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. Decode the next symbol.\n",
    "    2. Identical to the step 2.B of the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "<img src=\"data/adaptative-model-decoder.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Initially empty models\n",
    "\n",
    "* The smaller the number of symbols used by the model, the higher\n",
    "  the probabilities, and therefore, the better the compression ratios.\n",
    "* An initially empty model only stores the $\\mathtt{ESC}$(cape) symbol (a\n",
    "  symbol that it is used by the encoder only when a new symbol is\n",
    "  found and therefore, it must be part of the encoder's and decoder's models).\n",
    "* We assume that arithmetic coding is used and therefore, when we\n",
    "  input or output $c(s)$, we are transmitting $I(s)$ bits of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. Set the probability of the $\\mathtt{ESC}$ symbol to $1.0$ (and the probability of\n",
    "   the rest of the symbols to $0.0$).\n",
    "2. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ next symbol.\n",
    "    2. If $s$ has been found before, then:\n",
    "        1. Output $c(s)$ (encode).\n",
    "    3. Else:\n",
    "        1. Output $c(\\mathtt{ESC})$.\n",
    "        2. Output a raw symbol $s$.\n",
    "    4. Update $p(s)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. Identical to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $c(s)\\leftarrow $ next code-word.\n",
    "    2. Decode $s$.\n",
    "    3. If $s=\\mathtt{ESC}$, then:\n",
    "        1. Input a raw symbol $s$.\n",
    "    4. Update $p(s)$.\n",
    "    5. Output $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "TO-DO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "   \n",
    "## 4. Models with memory\n",
    "\n",
    "* In most cases, the probability of a symbol depends on its\n",
    "  neighborhood (context).\n",
    "* The higher the context, the higher the\n",
    "  accuracy of the predictions (probabilities), and therefore, the\n",
    "  lower the length of the code-words [[J. Cleary & I. Whitten]](https://scholar.google.es/scholar?hl=es&as_sdt=0%2C5&q=Data+Compression+using+Adaptive+Coding+and+Partial+String+Matching&btnG=).\n",
    "* Let ${\\cal C}[i]$ the last $i$ encoded symbols (the context) and\n",
    "  $p(s|{\\cal C}[i])$ the probability that the symbol $s$ follows\n",
    "  the context ${\\cal C}[i]$.\n",
    "* Let $k$ the maximal order of the prediction (i.e. the largest\n",
    "  number of symbols of ${\\cal C}[]$ that are going to be used as the\n",
    "  actual context). Notice that ${\\cal C}[0]=\\varnothing$ and the model\n",
    "  has no memory.\n",
    "* Let $r$ the size of the source alphabet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Encoder\n",
    "\n",
    "1. Create an empty model for every possible context of length $0\\le i \\le k$.\n",
    "2. Create an non-empty model for $k=-1$.\n",
    "3. While the input is not exhausted:\n",
    "    1. $s\\leftarrow$ Input ${\\log_2(r)}$ bits. # Input a symbol\n",
    "    2. $i\\leftarrow k$ (except for the first symbol, where\n",
    "       $i\\leftarrow 0$).\n",
    "    3. While $p(s|{\\cal C}[i])=0$ (it is the first time that $s$ follows\n",
    "       ${\\cal C}[i]$):\n",
    "        1. Output $\\leftarrow c(\\mathtt{ESC}|{\\cal C}[i])$.\n",
    "        2. Update $p(\\mathtt{ESC}|{\\cal C}[i])$.\n",
    "        3. Update $p(s|{\\cal C}[i])$ (insert $s$ into the ${\\cal C}[i]$ context).\n",
    "        4. $i\\leftarrow i-1$.\n",
    "    4. Output $\\leftarrow c(s|{\\cal C}[i])$. The symbols that were found in the models for\n",
    "       contexts with order $>i$ must be excluded of the actual (${\\cal C}[i]$) context-model because, for sure, $s$ is not any of them.\n",
    "    5. If $i\\ge 0$, update $p(s|{\\cal C}[i])$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "* Let $r=256$ the size of the source alphabet.\n",
    "\n",
    "* The probabilistic model $M[{\\cal C}[-1]]$ (for the special context\n",
    "  ${\\cal C}[-1]$) is static, non empty and has an special symbol $\\mathtt{EOF}$\n",
    "  (End Of File) that is going to be used when the compression has\n",
    "  finished:\n",
    "  $$M[{\\cal C}[-1]]=\\{0,1~1,1~\\cdots~\\mathtt{a},1~\\mathtt{b},1~\\cdots~255,1~\\mathtt{EOF},1\\}=A.$$\n",
    "  In a pair $(a,b)$, $a$ is the symbol and $b$ is its probability (represented by a counter of ocurrences of $s$ in the processed input until that moment).\n",
    "\n",
    "* $M[{\\cal C}[0]]$ is adaptative and empty:\n",
    "  $$M[{\\cal C}[0]]=\\{\\mathtt{ESC},1\\}.$$\n",
    "\n",
    "* In this example (for the sake of the simplicity), the maximal\n",
    "  order of the prediction $k=1$ (we only remember the previous\n",
    "  symbol). Therefore, there are $r=256$ probabilistic models:\n",
    "  $$M[{\\cal C}[1]]=\\{\\mathtt{ESC},1\\}, 0\\le {\\cal C}[1]\\le r.$$\n",
    "<br>\n",
    "\n",
    "* **Encoding of the first symbol ($\\mathtt{a}$)** (see the following figure):\n",
    "\n",
    "1. [3.A] $s\\leftarrow \\mathtt{a}$.\n",
    "2. [3.B] $i\\leftarrow 0$ (we don't know the previous symbol).\n",
    "3. [3.C] $p(\\mathtt{a}|{\\cal C}[0])=0$ (the context $M[C[0]]$ has only the $\\mathtt{ESC}$ symbol, $M[C[0]]=\\{\\mathtt{ESC},1\\}$).\n",
    "4. [3.C.a] Output $\\leftarrow c(\\mathtt{ESC}|{\\cal C}[0])$ (in fact, zero bits because \n",
    "    $l(c(\\mathtt{ESC}|{\\cal C}[0]))=0$).\n",
    "5. [3.C.b] Update $p(\\mathtt{ESC}|{\\cal C}[0])$ (now, $M[C[0]]=\\{\\mathtt{ESC},2\\}$).\n",
    "6. [3.C.c] Insert symbol $\\mathtt{a}$ into\n",
    "    $M[{\\cal C}[0]]=\\{\\mathtt{ESC},2~\\mathtt{a},1\\}$.\n",
    "7. [3.C.d] $i\\leftarrow -1$.\n",
    "8. [3.C] $p(\\mathtt{a}|{\\cal C}[-1])\\neq 0$.\n",
    "9. [3.D] Output $\\leftarrow c(\\mathtt{a}|{\\cal C}[-1])$ where\n",
    "    $p(\\mathtt{a}|{\\cal C}[-1]) = 1/(256+1)$.\n",
    "<br>\n",
    "\n",
    "* **Encoding of the second symbol ($\\mathtt{b}$)**:\n",
    "\n",
    "1. [3.A] $s\\leftarrow \\mathtt{b}$.\n",
    "2. [3.B] $i\\leftarrow 1$.\n",
    "3. [3.C] $p(\\mathtt{b}|{\\cal C}[1])=0$ because ${\\cal C}[1]=\\mathtt{a}$ andv$M[\\mathtt{a}]=\\{\\mathtt{ESC},1\\}$.\n",
    "4. [3.A.a] Output $\\leftarrow c(\\mathtt{ESC}|\\mathtt{a})$ with $l(c(\\mathtt{ESC}|\\mathtt{a}))=0$.\n",
    "5. [3.A.b] Update $p(\\mathtt{ESC})$ in $M[\\mathtt{a}]$ (now, $M[a]=\\{\\mathtt{ESC},2\\}$).\n",
    "6. [3.C.c] Insert $\\mathtt{b}$ into $M[\\mathtt{a}]=\\{\\mathtt{ESC},2~ \\mathtt{b},1\\}$.\n",
    "7. [3.C.d] $i\\leftarrow 0$.\n",
    "8. [3.C] $p(\\mathtt{b}|{\\cal C}[0])=0$ because $M[{\\cal C}[0]]=\\{\\mathtt{ESC},2~\\texttt{a},1\\}$.\n",
    "9. [3.C.a] Output $\\leftarrow c(\\mathtt{ESC}|{\\cal C}[0])$, where $p(\\mathtt{ESC}|{\\cal C}[0]) = 2/3$.\n",
    "10. [3.C.b] Update $p(\\mathtt{ESC}|{\\cal C}[0])$ (now, the count of $\\mathtt{ESC}$ is $3$).\n",
    "11. [3.C.c] Insert $\\mathtt{b}$ into $M[{\\cal C}[0]] = \\{\\mathtt{ESC},3~\\mathtt{a},1~\\mathtt{b},1\\}$.\n",
    "12. [3.C.d] $i\\leftarrow -1$.\n",
    "13. [3.C] $p(\\mathtt{b}|{\\cal C}[-1])\\neq 0$.\n",
    "14. [3.D] Output $\\leftarrow c(\\mathtt{b}|{\\cal C}[-1])$ where $p(\\mathtt{b}|{\\cal C}[-1]) = 1/r$. The symbol $\\mathtt{a}$ has been excluded of the calculus of the probability of $\\mathtt{b}$ for context $C[-1]$ because $\\mathtt{a}\\in M[{\\cal C}[0]] = \\{\\mathtt{ESC},3~ \\mathtt{a},1~ \\mathtt{b},1\\}$.\n",
    "\n",
    "<img src=\"data/PPM_example.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Decoder\n",
    "\n",
    "1. Equal to the step 1 of the encoder.\n",
    "2. While the input is not exhausted:\n",
    "    1. $i\\leftarrow k$ (except for the first symbol, where $i\\leftarrow 0$).\n",
    "    2. $s\\leftarrow$ next decoded symbol.\n",
    "    3. While $s=\\mathtt{ESC}$:\n",
    "        1. Update $p(\\mathtt{ESC}|{\\cal C}[i])$.\n",
    "        2. $i\\leftarrow i-1$.\n",
    "        3. $s\\leftarrow$ next decoded symbol.\n",
    "    4. Update $p(s|{\\cal C}[i])$.\n",
    "    5. While $i<k$:\n",
    "        1. $i\\leftarrow i+1$.\n",
    "        2. Update $p(s|{\\cal C}[i])$ (insert $s$ into the ${\\cal C}[i]$ context)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Lab\n",
    "TO-DO"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
